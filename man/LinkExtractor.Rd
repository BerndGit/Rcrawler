% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/LinkExtractor.R
\name{LinkExtractor}
\alias{LinkExtractor}
\title{LinkExtractor}
\usage{
LinkExtractor(url, id, lev, IndexErrPages, Useragent, Timeout = 5,
  URLlenlimit = 255, urlExtfilter, statslinks = FALSE, encod, urlbotfiler,
  removeparams)
}
\arguments{
\item{url}{character, url to fetch and extract links.}

\item{id}{numeric, an id to identify a specific web page in a website collection, it's auto-generated by default}

\item{lev}{numeric, the depth level of the web page, auto-generated by the \code{Rcrawler} function.}

\item{IndexErrPages}{a vector of html error code-statut page to index, by default it's c(200), to include 404 and 403 pages c(404,403)}

\item{Useragent}{, default to "Rcrawler"}

\item{Timeout}{,default to 5s}

\item{URLlenlimit}{interger, the url character length limit to index, default to 255 characters (to avoid spider traps)}

\item{urlExtfilter}{character vector, the list of file extensions to exclude from indexing, by dfault a large list is defined (html pages only are permitted) in order to prevent large files downloading; To define your own use c(ext1,ext2,ext3 ...)}

\item{statslinks}{boolean, specifies if input and output links shoud be counted, work only when the function is called from the main function \code{scrawler}}

\item{urlbotfiler}{character vector , directories/files restricted by robot.txt}

\item{encoding}{character, specify the encoding of th web page}
}
\value{
return a list of two elements, the first is a list containing the page detail (url, encoding-type, content-type, content ... etc) and the second is a character-vector containing the list of urls discovered in that page
}
\description{
A function that take a -charachter_ url as input, fetch the web page, gets its detail, and extract all links following a set of rules .
}
\details{
xxx
}
\author{
salim khalil
}
\seealso{
\code{other function}
}

