% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/robotparser.R
\name{RobotParser}
\alias{RobotParser}
\title{RobotParser}
\usage{
RobotParser(website, useragent)
}
\arguments{
\item{website}{character, url of the website which rules have to be extracted  .}

\item{Useragent}{character, the useragent of the crawler}
}
\value{
return a list of three elements, the first is a character vector of Disallowed directories, the third is a Boolean value which is TRUE if the user agent of the crawler is blocked.
}
\description{
This function fetch and parse robots.txt file of the website which is specified in the first argument and return the list of correspending rules .
}
\details{
xxx
}
\seealso{
\code{other function}
}

